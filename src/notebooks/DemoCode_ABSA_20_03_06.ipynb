{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DemoCode-ABSA-20-03-06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1o8MCPAHNiM",
        "colab_type": "text"
      },
      "source": [
        "Demo for aspect-based Sentiment Analysis shown at the jambit CoffeeTalks on 6th of March, 2020.\n",
        "\n",
        "Based mainly on [Ben Trevett's PyTorch Sentiment Analysis](https://github.com/bentrevett/pytorch-sentiment-analysis). The training and testing data is a prepared csv version of the restaurant data from the SemEval 2014 task on Aspect-based Sentiment Analysis.\n",
        "\n",
        "(c) Wiltrud Kessler\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFUuo5iXJ4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv1QyddA4lf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pytorch and torchtext libraries\n",
        "from torchtext import data\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYiJ5WWU42R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task settings\n",
        "\n",
        "# The data we use has sentiment polarity annotations and aspect annotations.\n",
        "# There are 4 possible sentiment labels, '1', '-1', '0', and 'conflict'.\n",
        "# There are two types of aspect annotations in the data, 'category' (5 different aspect categories) and 'terms' (actual aspect words).\n",
        "# Chose here which setting to run: \n",
        "# Demo 1: Polarity classification -> labeltype 'category', use_aspect_label False. Play around with polarities, if you like.\n",
        "# Demo 2: Aspect category classification -> labeltype 'category', use_aspect_label True.\n",
        "# Demo 3: Aspect term classification -> labeltype 'term', use_aspect_label True.\n",
        "labeltype = 'category'\n",
        "polarities = ['1', '-1', '0', 'conflict']\n",
        "use_aspect_label = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZuAdyG8Wpl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the data from csv\n",
        "\n",
        "ID = data.Field()\n",
        "TEXT = data.Field()\n",
        "ASPECT = data.Field()\n",
        "POLARITY = data.Field()\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# Select the columns of the csv file that we want to use\n",
        "# field ->   sent.id          text         ex.id          aspect           polarity\n",
        "if use_aspect_label:\n",
        "  fields = [(None, None), ('text', TEXT), (None, None), ('label', LABEL), (None, None)] # Use aspect as label\n",
        "else:\n",
        "  fields = [(None, None), ('text', TEXT), (None, None), (None, None), ('label', LABEL)] # Use polarity as label\n",
        "\n",
        "# The data is already split into training/validation/test to load with the corresponding names\n",
        "prefix = 'semeval2014_restaurants_' + labeltype + \"_\" + \".\".join(polarities)\n",
        "print(f'Loading data from {prefix}')\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = '/content/drive/My Drive/semeval',\n",
        "                                        train = prefix + '_train.csv',\n",
        "                                        validation = prefix + '_val.csv',\n",
        "                                        test = prefix + '_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")\n",
        "\n",
        "# Check if we loaded the right data by the number of examples\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "\n",
        "# Print some examples\n",
        "print(vars(train_data.examples[1]))\n",
        "print(vars(valid_data.examples[0]))\n",
        "print(vars(test_data.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lVIUH4A5Pht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the vocabulary\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "MAX_LABEL_SIZE = 25_000 # take all\n",
        "\n",
        "# Build the vocabulary only over the training data (test data is unknown)\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data, max_size = MAX_LABEL_SIZE)\n",
        "\n",
        "# Look at the numbers a bit\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Most frequent vocabulary words: {TEXT.vocab.freqs.most_common(20)}\")\n",
        "\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
        "print(f\"Classes distribution: {LABEL.vocab.freqs}\")\n",
        "\n",
        "if use_aspect_label and labeltype == 'term':\n",
        "  print(f\"Most frequent class words: {LABEL.vocab.freqs.most_common(MAX_LABEL_SIZE)}\")\n",
        "  num = sum([x[1] for x in LABEL.vocab.freqs.most_common(MAX_LABEL_SIZE)])\n",
        "  if MAX_LABEL_SIZE < len(LABEL.vocab):\n",
        "    print(f\"Words with real label: {num} Words with default label {len(train_data)-num}\")\n",
        "\n",
        "# Move computations to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# The algorithm needs to be able to iterate over the data later on,\n",
        "# these iterators are defined here\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=False,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZjiVT9B5mEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the machine learning (Recurrent Neural Network and evaulation)\n",
        "\n",
        "# The RNN itself\n",
        "class RNN(nn.Module): \n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "# Define the metric for evaluation = accuracy\n",
        "def categorical_accuracy(preds, y, labels):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y) # check if it is correct\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]]), confusion_matrix(y, max_preds, labels=labels)\n",
        "\n",
        "# Define training of the model\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_matrix = 0\n",
        "    model.train() # put the model into training mode\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad() # remove gradients from last round\n",
        "        predictions = model(batch.text)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc, matrix = categorical_accuracy(predictions, batch.label, range(0,model.fc.out_features))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_matrix += matrix\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_matrix\n",
        "\n",
        "# Define evaluation of the model\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0    \n",
        "    epoch_matrix = 0\n",
        "    model.eval() # put the model into evaluation mode (no weights are changed)\n",
        "    \n",
        "    with torch.no_grad():    # enhance efficiency by telling PyTorch not to update gradients\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc, matrix = categorical_accuracy(predictions, batch.label, range(0,model.fc.out_features))\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_matrix += matrix\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_matrix\n",
        "\n",
        "# Debug function that counts the parameters of the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Debug function to show how much time one training iteration takes\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J9kR0Yi55gJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actually define and train the model\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab) # each word is an input dimension\n",
        "EMBEDDING_DIM = 100 # this number falls from the sky and may be tuned ;)\n",
        "HIDDEN_DIM = 256 # this number falls from the sky and may be tuned ;)\n",
        "OUTPUT_DIM = len(LABEL.vocab) # each label is an output dimension\n",
        "N_EPOCHS = 10 # How often to iterate through all examples\n",
        "\n",
        "# Get an instance of our RNN class\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "print(model)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "# Set other parameters for the network (optimizer, loss function)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Move calculation to GPU, if we have one\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Train the model a few times on the training data and evaluate it\n",
        "# on the validation data. Save the best model.\n",
        "print(\"Labels (in order): \" + str(LABEL.vocab.itos))\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc, train_matrix = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_matrix = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'absa1.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}      | Train Accuracy: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValidation Loss: {valid_loss:.3f} | Validation Accuracy: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0oA6VWu59ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate the model on the test data\n",
        "\n",
        "model.load_state_dict(torch.load('absa1.pt'))\n",
        "\n",
        "test_loss, test_acc, test_matrix = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Accuracy: {test_acc*100:.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZyBm8_Q6ANT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try out a few examples by hand\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Sentence tokenization (the same that has been done for the train and test data)\n",
        "def custom_tokenize(text):\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    words = [word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "# Give the sentence to the model and receive a prediction for it\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval() # Put model in eval mode\n",
        "    tokenized = custom_tokenize(sentence)\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    predictions = torch.sigmoid(model(tensor))\n",
        "    max_prediction = predictions.argmax(dim = 1)\n",
        "    return predictions.tolist(), max_prediction.item()\n",
        "\n",
        "# POLARITY examples\n",
        "if not use_aspect_label:\n",
        "  # Label here is human-readable, i.e. 0 is neutral, 1 is positive, -1 is negative\n",
        "  sentences = [\n",
        "      (\"The food was delicious.\", \"1\"),\n",
        "      (\"This is by far my favorite place in the neighborhood\", \"1\"),\n",
        "      (\"The sushi was awful!\", \"-1\"),\n",
        "      (\"Service was prompt, friendly and great.\", \"1\"),\n",
        "      (\"The website and rating makes this place look wonderful but in reality it was very disappointing.\", \"conflict\"),\n",
        "      (\"I know because I live nearby.\", \"0\")\n",
        "  ]\n",
        "\n",
        "# ASPECT CATEGORY examples\n",
        "if labeltype == 'category' and use_aspect_label:\n",
        "   sentences = [\n",
        "      (\"The food was delicious.\", \"food\"),\n",
        "      (\"This is by far my favorite place in the neighborhood\", \"anecdotes miscellaneous\"),\n",
        "      (\"The sushi was awful!\", \"food\"),\n",
        "      (\"Service was prompt, friendly and great.\", \"service\"),\n",
        "      (\"in the neighborhood it is well worth the price you pay for them.\", \"price\")\n",
        "   ]\n",
        "\n",
        "# ASPECT TERM examples\n",
        "if labeltype == 'term' and use_aspect_label:\n",
        "   sentences = [\n",
        "      (\"The food was delicious.\", \"food\"),\n",
        "      (\"The sushi was awful!\", \"sushi\"),\n",
        "      (\"My pick for best pizza restaurant anywhere!\", \"pizza\"),\n",
        "      (\"The atmosphere isn't the greatest , but I suppose that's how they keep the prices down .\", \"atmosphere\"),\n",
        "      (\"The atmosphere isn't the greatest , but I suppose that's how they keep the prices down .\", \"prices\"),\n",
        "      (\"the desert was good.\", \"desert\")\n",
        "   ]\n",
        "\n",
        "for s in sentences:\n",
        "   prediction = predict_sentiment(model, s[0])\n",
        "   predicted_label = LABEL.vocab.itos[prediction[1]]\n",
        "   print(f'Sentence: {s[0]}\\n   {str(s[1]) == predicted_label}! - Gold label: {s[1]} - Predicted label: {predicted_label}') # Probabilities: {prediction[0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqT6EZt1RgoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = (\"Service was very prompt but slightly rushed.\", \"service\")\n",
        "prediction = predict_sentiment(model, s[0])\n",
        "predicted_label = LABEL.vocab.itos[prediction[1]]\n",
        "print(f'Sentence: {s[0]}\\n   {str(s[1]) == predicted_label}! - Gold label: {s[1]} - Predicted label: {predicted_label}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}